---
title: ❏ Notes to Self 001 - Projecting Intelligence
date: March 14, 2023
tags: "ai, being human"
description: "Research in progress from the studio of Andrea Mignolo."
keywords: "research, lab notes, leadership, being human, technology, design, cosmology, worlding"
image: "og/mm-og-1.jpg"
---

Continuing to collect links and insights that are helping me make sense of the most recent season of **Is Artificial Intelligence Intelligent**? Of course the day after I wrote the [ChatGPT edition of Kitchen Party](../../kp/005), Intelligencer published [You Are Not a Parrot](https://nymag.com/intelligencer/article/ai-artificial-intelligence-chatbots-emily-m-bender.html) which says a lot of what I was trying to say but much better, and with more experts.

Then John Maeda sent out the [March 2023 #DesignInTech briefing](https://www.linkedin.com/pulse/simons-scissors-weizenbaums-wisdom-nicholas-dr-john-maeda/), in which he includes a passage from [Computer Power and Human Reason](http://blogs.evergreen.edu/cpat/files/2013/05/Computer-Power-and-Human-Reason.pdf), penned by one Dr. Joseph Weizenbaum who apparently invented the chatbot back in 1967, so definitely has some street cred:

> I knew of course that people form all sorts of emotional bonds to machines, for example, to musical instruments, motorcycles, and cars. And I knew from long experience that the strong emotional ties many programmers have to their computers are often formed after only short exposures to their machines. What I had not realized is that extremely short exposures to a relatively simple computer program could induce powerful delusional thinking in quite normal people.
This insight led me to attach new importance to questions of the relationship between the individual and the computer, and hence to resolve to think about them.

Once more for everyone in the back, **_extremely short exposures to a relatively simple computer program could induce powerful delusional thinking in quite normal people_**. I mean, this feels like what is happening, no?

To keep our heads spinning, [GPT-4](https://openai.com/research/gpt-4) was just released, along with a [white paper](https://arxiv.org/abs/2303.08774) I haven't read (yet) but [it sure doesn't sound very open](https://twitter.com/emilymbender/status/1636089802260111360).

My working assumption right now is that to whatever extent intelligence is being attributed to these tools, we are dealing with projection. Because if you look at the larger zeitgeist and polarizations that are happening in this country (around the world, really) and the intense desire to dehumanize a large part of the planet (I mean, the same desire that's been around for at least 500 years with the rampant violence and subjugation of humans and nature), what is with the urge to humanize a Large Language Model? And if you took the other side and assumed the projections were based on positive qualities, then we run the risk of transforming these models into gods. But either way you look at it, the one thing that remains true for projections is that they function to absolve us of any responsibility in the matter. And that is what scares me the most. 
