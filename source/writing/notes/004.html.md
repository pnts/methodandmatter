---
title: "AI: In Service of Capital or Labor?"
date: May 5, 2023
tags: "leadership"
description: "I had assumed AI was going to be a tool for those who control capital, but it might also serve workers."
label: Notes to Self
keywords: "research, lab notes, leadership, being human, technology, design, cosmology, worlding"
image: "og/og-symbol.png"
status: seedling
---

It has been interesting watching the debate, development, and understanding of how AI-based tools will impact society. Who will benefit? Who will be left behind? What does it mean? What is our role in shaping and tending to it? What is possible? Can we resist what feels like an unavoidable and increasingly dystopian future?

Two things that kind of bookmark what I am seeing are Ted Chiang’s most recent piece in in the New Yorker, “[Will AI Become the New McKinsey?](https://www.newyorker.com/science/annals-of-artificial-intelligence/will-ai-become-the-new-mckinsey)” in which he offers [another compelling metaphor](https://www.newyorker.com/tech/annals-of-technology/chatgpt-is-a-blurry-jpeg-of-the-web) by which to think about AI: as a management consulting firm. What does this actually mean? The two qualities called out in the article are 1) the use of management consulting firms by organizations to evade responsibility and 2) how these management consulting firms work as “capital’s willing executioners”. Each of these qualities work in service of increasing shareholder value in the hands of private individuals who profit off the effort of others.

> As it is currently deployed, A.I. often amounts to an effort to analyze a task that human beings perform and figure out a way to replace the human being. Coincidentally, this is exactly the type of problem that management wants solved. As a result, A.I. assists capital at the expense of labor. There isn’t really anything like a labor-consulting firm that furthers the interests of workers. Is it possible for A.I. to take on that role? Can A.I. do anything to assist workers instead of management?

This echoes Andrew Feenberg’s Critical Theory of Technology and the ensuing explication of “capitalist technology” in the fourth moment of instrumentation,  _decontextualization and systemization_. In this dialectic moment technology is combined and reinserted back into the environment in service of hegemonic interests:

> Decontextualization is of course only the starting point in technical development since the decontextualized elements must be combined with each other to be useful. The resulting device must then be related to other devices and to its natural environment. “Systemization” is the secondary instrumentalization where technical design addresses a sufficiently wide range of contexts. Capitalism does greatly enlarge that range insofar as devices form each others’ context, integrating enormous numbers of them in tightly coupled networks. This gives rise to what I call “system-centered design,” the typical design strategy of modern societies. However, where the well-being of workers and nature are concerned, it limits those contexts as much as possible for the sake of control and profits.” - p 181, Transforming Technology

I love the questions that are forming here - what does AI look like when it is in service of workers? How can networks be reconfigured in service of nature and labor?

On the other hand, we have the document that leaked out of Google this week, “[We Have No Moat, and Neither Does AI](https://www.semianalysis.com/p/google-we-have-no-moat-and-neither).” In the analysis an internal researcher looks at the pace of LLM innovation in the open source community, and how in the weeks and months since LLaMA was leaked, individuals have pushed the pace of innovation and minification to the point personalized LLMs can be run on a laptop. In this world, the value corporate controlled LLMs offer diminishing (if any) returns for customers who can get similar power for free. AI is now in the hands of the masses, freed from corporate control. The paper acknowledges this reality and suggests that organizations like Google and AI embrace what is happening and relinquish some control over their models.

> The more tightly we control our models, the more attractive we make open alternatives. Google and OpenAI have both gravitated defensively toward release patterns that allow them to retain tight control over how their models are used. But this control is a fiction. Anyone seeking to use LLMs for unsanctioned purposes can simply take their pick of the freely available models.

Who knows where this will all go, but I am watching the open source space and pace closely. Maybe AI can push things in favor of labor after all.
